{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Create a sample dataframe with a categorical variable\n",
    "df = pd.DataFrame({'Color': ['Red', 'Green', 'Blue', 'Green', 'Red']})\n",
    "print(\"Original dataframe:\")\n",
    "print(df)\n",
    "\n",
    "# One-hot encoding using sklearn OneHotEncoder\n",
    "one_hot_encoder = OneHotEncoder()\n",
    "one_hot_encoded = one_hot_encoder.fit_transform(df[['Color']])\n",
    "one_hot_df = pd.DataFrame(one_hot_encoded.toarray(), columns=one_hot_encoder.get_feature_names_output(['Color']))\n",
    "print(\"\\nOne-hot encoded dataframe:\")\n",
    "print(one_hot_df)\n",
    "\n",
    "# Label encoding using sklearn LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "df['Color_LabelEncoded'] = label_encoder.fit_transform(df['Color'])\n",
    "print(\"\\nLabel encoded dataframe:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Min Max Scalling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/data-normalization-with-pandas-and-scikit-learn-7c1cc6ed6475"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.  ]\n",
      " [0.25 0.25]\n",
      " [0.5  0.5 ]\n",
      " [1.   1.  ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
    "scaler = MinMaxScaler()\n",
    "print(scaler.fit_transform(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Z-score Scalling(Standard Scalling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# create a scaler object\n",
    "std_scaler = StandardScaler()\n",
    "std_scaler\n",
    "# fit and transform the data\n",
    "df_std = pd.DataFrame(std_scaler.fit_transform(df_cars), columns=df_cars.columns)\n",
    "\n",
    "df_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyreadstat #to read and write sas (sas7bdat, sas7bcat, xport), spps (sav, zsav, por)\n",
    "#and stata (dta) data files into/from pandas dataframes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setting environment to ignore future warnings\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# Changing default pandas setting to custom\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams[\"figure.figsize\"] = (14, 7)\n",
    "pd.set_option('max_columns', 30)\n",
    "pd.set_option('max_rows', 20)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform all EDA\n",
    "def perform_eda(df, name=\"\"):\n",
    "    # Printing basic detail of data like name, size, shape\n",
    "    print(f\"EDA of {str(name)} Data....\")\n",
    "    print(f\"Size {df.size}\")\n",
    "    print(f\"Columns {df.shape[1]}\")\n",
    "    print(f\"Records {df.shape[0]}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Printing top 5 records of data\n",
    "    print(\"First Look of Data....\")\n",
    "    display(df.head())\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Getting Numerical and Categorical columns Separately\n",
    "    cat_cols = df.select_dtypes(np.object).columns\n",
    "    num_cols = df.select_dtypes(np.number).columns\n",
    "\n",
    "    # Printing the Numerical columns\n",
    "    print(\"Dataset has following Numerical columns...\")\n",
    "    for i, j in enumerate(num_cols):\n",
    "        print(f\" {i+1}) {j}\")\n",
    "\n",
    "    # Printing the Categorical columns\n",
    "    print(\"\\n\\nDataset has following Categorical columns...\")\n",
    "    for i, j in enumerate(cat_cols):\n",
    "        print(f\" {i+1}) {j}\")\n",
    "    \n",
    "    # Printing info of data like data type, non null values\n",
    "    print(\"=\"*50)\n",
    "    print(\"Information of Data....\")\n",
    "    print(df.info())\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Displaying statistical properties of data like mean, median, max, min\n",
    "    print(\"Statistical Properties of Data....\")\n",
    "    display(df.describe(include=\"all\"))\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Displaying correlation of numerical features\n",
    "    corr = df.corr(method=\"kendall\").style.background_gradient(\"YlOrRd_r\")\n",
    "    print(\"Correlation of Numerical features....\")\n",
    "    display(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the name of columns\n",
    "print(\"Dataset has following columns...\")\n",
    "for i, j in enumerate(df.columns):\n",
    "    print(f\" {i+1}) {j}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Numerical and Categorical columns Separately\n",
    "cat_cols = df.select_dtypes(np.object).columns\n",
    "num_cols = df.select_dtypes(np.number).columns\n",
    "\n",
    "# Printing the Numerical columns\n",
    "print(\"Dataset has following Numerical columns...\")\n",
    "for i, j in enumerate(num_cols):\n",
    "    print(f\" {i+1}) {j}\")\n",
    "\n",
    "# Printing the Categorical columns\n",
    "print(\"\\n\\nDataset has following Categorical columns...\")\n",
    "for i, j in enumerate(cat_cols):\n",
    "    print(f\" {i+1}) {j}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage on bar\n",
    "def per_on_bar(feature, title=\"\", limited=False, n=10):\n",
    "    print(\"Total unique values are: \", len(feature.value_counts()), \"\\n\\n\")\n",
    "    print(\"Category\\tValue\\n\")\n",
    "    if limited:\n",
    "        data = feature.value_counts()[0:n]\n",
    "    else:\n",
    "        data = feature.value_counts()\n",
    "    print(data)\n",
    "    categories_num = len(data)\n",
    "    #plotting bar-plot and pie chart\n",
    "    sns.set_style('darkgrid')\n",
    "    plt.figure(figsize=(16,5))\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xticks(rotation=45)\n",
    "    plot = sns.barplot(x=data.index, y=data.values, edgecolor=\"black\", palette=sns.palettes.color_palette(\"icefire\"))\n",
    "    total = len(feature)\n",
    "    for p in plot.patches:\n",
    "        percentage = '{:.1f}%'.format(100 * p.get_height()/total)\n",
    "        x = p.get_x() + p.get_width() / 2 - 0.08\n",
    "        y = p.get_y() + p.get_height()\n",
    "        plot.annotate(percentage, (x, y), size = 12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to explore continous features\n",
    "def explore_feature(feature_name):\n",
    "    # Printing details\n",
    "    print(f\"Exploring {str(feature_name).upper()}........\")\n",
    "    print(f\"Mean of {feature_name}     : {df_train[feature_name].mean()}\")\n",
    "    print(f\"Median of {feature_name}   : {df_train[feature_name].median()}\")\n",
    "    print(f\"Mode of {feature_name}     : {df_train[feature_name].mode()}\")\n",
    "    print(f\"Variance of {feature_name} : {df_train[feature_name].var()}\")\n",
    "    print(f\"Skewness of {feature_name} : {df_train[feature_name].skew()}\")\n",
    "    print(f\"Maximum of {feature_name}  : {df_train[feature_name].max()}\")\n",
    "    print(f\"Minimum of {feature_name}  : {df_train[feature_name].min()}\")\n",
    "    temp = df_train[[feature_name, \"Sepssis\"]]\n",
    "    temp.Sepssis = temp.Sepssis.map({\"Negative\": 0, \"Positive\": 1})\n",
    "    corr = temp.corr().iloc[0, 1]\n",
    "    print(f\"Correlation with the target feature : {corr}\")\n",
    "    temp = df_train[df_train[feature_name] > df_train[feature_name].mean()]\n",
    "    ratio = len(temp[temp.Sepssis == \"Positive\"])/len(temp)\n",
    "    print(f\"Ratio of being Sepssis for the patient whose {feature_name} is more than average value : {ratio}\")\n",
    "    temp = df_train[df_train[feature_name] < df_train[feature_name].mean()]\n",
    "    ratio = len(temp[temp.Sepssis == \"Positive\"])/len(temp)\n",
    "    print(f\"Ratio of being Sepssis for the patient whose {feature_name} is less than average value : {ratio}\")\n",
    "    \n",
    "    # Drawing plots\n",
    "    plt.figure(figsize=(17, 4))\n",
    "    fig=plt.figure(figsize=(17, 4))\n",
    "    plt.subplot(131)\n",
    "    sns.kdeplot(df_train[feature_name])\n",
    "    \n",
    "    plt.subplot(132)\n",
    "    sns.boxplot(df_train[feature_name])\n",
    "    \n",
    "    plt.subplot(133)\n",
    "    sns.scatterplot(df_train.index, df_train[feature_name], hue=df_train.Sepssis)\n",
    "    fig.suptitle(\"Plots for Whole data\")\n",
    "    plt.show()\n",
    "    \n",
    "    temp = df_train[df_train.Sepssis == \"Positive\"]\n",
    "    # Drawing plots\n",
    "    plt.figure(figsize=(17, 4))\n",
    "    fig=plt.figure(figsize=(17, 4))\n",
    "    plt.subplot(131)\n",
    "    sns.kdeplot(temp[feature_name])\n",
    "    \n",
    "    plt.subplot(132)\n",
    "    sns.boxplot(temp[feature_name])\n",
    "    \n",
    "    plt.subplot(133)\n",
    "    sns.scatterplot(temp.index, temp[feature_name], hue=temp.Sepssis)\n",
    "    fig.suptitle(\"Plots for Sepssis Patients\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plots(feature, title=\"\", limited=False, n=10):\n",
    "    print(\"Total unique values are: \", len(feature.value_counts()), \"\\n\\n\")\n",
    "    print(\"Category\\tValue\\n\")\n",
    "    if limited:\n",
    "        data = feature.value_counts()[0:n]\n",
    "    else:\n",
    "        data = feature.value_counts()\n",
    "    print(data)\n",
    "    categories_num = len(data)\n",
    "    #plotting bar-plot and pie chart\n",
    "    sns.set_style('darkgrid')\n",
    "    plt.figure(figsize=(16,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xticks(rotation=45)\n",
    "    plot = sns.barplot(x=data.index, y=data.values, edgecolor=\"white\", palette=sns.palettes.color_palette(\"icefire\"))\n",
    "    total = len(feature)\n",
    "    for p in plot.patches:\n",
    "        percentage = '{:.1f}%'.format(100 * p.get_height()/total)\n",
    "        x = p.get_x() + p.get_width() / 2 - 0.08\n",
    "        y = p.get_y() + p.get_height()\n",
    "        plot.annotate(percentage, (x, y), size = 12)\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    labels = data.index\n",
    "    plt.pie(x=data, autopct=\"%.1f%%\", explode=[0.02]*categories_num, labels=labels, pctdistance=0.5)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bar_plot(x, y, title, xlable=None, ylable=None):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    sns.barplot(x, y)\n",
    "    plt.title(title, fontsize=18)\n",
    "    plt.xlabel(xlable, fontsize=14)\n",
    "    plt.ylabel(ylable, fontsize=14)\n",
    "    plt.xticks(rotation=65)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plots(x, y, title=\"\", xlable=\"\", ylable=\"\", palette=\"Blues_d\"):    \n",
    "    #plotting bar-plot and pie chart\n",
    "    sns.set_style('darkgrid')\n",
    "    plt.figure(figsize=(16,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    sns.barplot(x, y, palette=palette)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xlabel(xlable, fontsize=12)\n",
    "    plt.ylabel(ylable, fontsize=12)\n",
    "    plt.xticks(rotation=65)\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    categories_num = len(x)\n",
    "    plt.pie(x=y, autopct=\"%.1f%%\", explode=[0.08]*categories_num, labels=x, pctdistance=0.5)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wordcloud(text):\n",
    "    from wordcloud import WordCloud\n",
    "    # Generating WordCloud\n",
    "    comment_words = ''\n",
    "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "    # iterate through the csv file\n",
    "    for val in text:\n",
    "        # typecaste each val to string\n",
    "        val = str(val)\n",
    "        # split the value\n",
    "        tokens = val.split()\n",
    "\n",
    "        # Converts each token into lowercase\n",
    "        for i in range(len(tokens)):\n",
    "            tokens[i] = tokens[i].lower()\n",
    "\n",
    "        comment_words += \" \".join(tokens)+\" \"\n",
    "\n",
    "    wordcloud = WordCloud(width = 1200, height = 700, background_color ='black',\n",
    "          stopwords = stopwords,\n",
    "          min_font_size = 10).generate(comment_words)\n",
    "\n",
    "    # plot the WordCloud image\n",
    "    plt.figure(figsize = (12, 7), facecolor = None)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad = 0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make plots for numerical feature\n",
    "def make_plots(feature):\n",
    "    # Setting figure size\n",
    "    plt.figure(figsize=(13, 5))\n",
    "    \n",
    "    # Making KDE plot\n",
    "    plt.subplot(131)\n",
    "    sns.kdeplot(feature)\n",
    "    \n",
    "    # Making boxen plot\n",
    "    plt.subplot(132)\n",
    "    sns.boxenplot(feature)\n",
    "    \n",
    "    # Scatter plot to check relation with target\n",
    "    plt.subplot(121)\n",
    "    sns.scatterplot(feature, df.High)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balancing data\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sampler = SMOTE()\n",
    "X, y = sampler.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A custom function defined in order to fine-tune the cleaning of the input text. This function is highly dependent on each usecase.\n",
    "# Note: Only include misspelling or abbreviations of commonly used words.\n",
    "#       Including many minimally present cases would negatively impact the performance. \n",
    "def clean_abbreviation(token):\n",
    "    if token == 'u':\n",
    "        return 'you'\n",
    "    if token == 'r':\n",
    "        return 'are'\n",
    "    if token == 'some1':\n",
    "        return 'someone'\n",
    "    if token == 'yrs':\n",
    "        return 'years'\n",
    "    if token == 'hrs':\n",
    "        return 'hours'\n",
    "    if token == 'mins':\n",
    "        return 'minutes'\n",
    "    if token == 'secs':\n",
    "        return 'seconds'\n",
    "    if token == 'pls' or token == 'plz':\n",
    "        return 'please'\n",
    "    if token == '2morow':\n",
    "        return 'tomorrow'\n",
    "    if token == '2day':\n",
    "        return 'today'\n",
    "    if token == '4got' or token == '4gotten':\n",
    "        return 'forget'\n",
    "    if token == 'amp' or token == 'quot' or token == 'lt' or token == 'gt' or token == 'Â½25':\n",
    "        return ''\n",
    "    return token\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "STOPWORDS = nltk.corpus.stopwords.words(\"english\") \n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# Function to clean the data\n",
    "def clean_data(txt):\n",
    "    txt = txt.lower()                             # lowering text\n",
    "    txt = re.sub(r'#', '', txt)                   # Removing hashtags\n",
    "    txt = re.sub(r'@[A-Za-z0-9]+', '', txt)       # Removing Mentions\n",
    "    txt = re.sub(r'https?:\\/\\/\\S+', '', txt)      # Removing Links\n",
    "    txt = re.sub(r'RT[\\s]+', '', txt)             # Removing Retweets\n",
    "    txt = re.sub(r'\\n', ' ', txt)                 # Removing Newline\n",
    "    txt = re.sub(r\"[^a-zA-Z0-9]\",\" \", txt)        # Removing all special characters\n",
    "    txt = \" \".join([clean_abbreviation(i) for i in txt.split()])                           # Checking for abbreviations\n",
    "    txt = \" \".join([lemmatizer.lemmatize(i) for i in txt.split() if i not in STOPWORDS])   # Removing stopwrods and applying lemmatization\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Next Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try to check the percentage of missing values,unique values,percentage of one catagory values and type against each column.\n",
    "def statistics(df):\n",
    "    stats = []\n",
    "    for col in df.columns:\n",
    "        stats.append((col, df[col].nunique(), df[col].isnull().sum(), df[col].isnull().sum() * 100 / df.shape[0], df[col].dtype))\n",
    "\n",
    "    stats_df = pd.DataFrame(stats, columns=['Feature', 'Unique_values', 'Missing values', 'Percentage of Missing Values', 'Data Type'])\n",
    "    stats_df.set_index('Feature', drop=True, inplace=True)\n",
    "    stats_df.drop(stats_df[stats_df['Missing values'] == 0].index, axis=0, inplace=True)\n",
    "    stats_df.sort_values('Percentage of Missing Values', ascending=False, inplace=True)\n",
    "    return stats_df\n",
    "\n",
    "statistics(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import normaltest\n",
    "\n",
    "# Function to make kde plot\n",
    "def make_kde(feature):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    sns.kdeplot(feature)\n",
    "    plt.show()\n",
    "\n",
    "# Iterating Numerical columns\n",
    "for i in num_cols:\n",
    "    # Finding Normality of feature\n",
    "    st, p = normaltest(train[i].dropna())\n",
    "    \n",
    "    # Checking if normal or not\n",
    "    if p > 0.05:\n",
    "        print(\"Normal\")\n",
    "        # Filling with mean if normal\n",
    "        train[i].fillna(train[i].mean(), inplace=True)\n",
    "        test[i].fillna(test[i].mean(), inplace=True)\n",
    "    else:\n",
    "        print(\"Not Normal\")\n",
    "        # Filling with median if not normal\n",
    "        train[i].fillna(train[i].median(), inplace=True)\n",
    "        test[i].fillna(test[i].median(), inplace=True)  \n",
    "    \n",
    "    # making kde plot\n",
    "    make_kde(train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n = df.copy()\n",
    "\n",
    "# lets try to remove the outliers\n",
    "for x in df[cols].columns.tolist():\n",
    "    q75,q25 = np.percentile(df.loc[:,x],[75,25])\n",
    "    intr_qr = q75-q25\n",
    " \n",
    "    max = q75+(1.5*intr_qr)\n",
    "    min = q25-(1.5*intr_qr)\n",
    " \n",
    "    df_n.loc[df[x] < min,x] = np.nan\n",
    "    df_n.loc[df[x] > max,x] = np.nan\n",
    "\n",
    "# lets try to check the sum of count of NULL values/outliers in each column of the dataset\n",
    "print(df_n.isnull().sum())\n",
    "df1 = df_n.dropna(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_n.dropna(axis = 0)\n",
    "print(df1.isnull().sum())\n",
    "print()\n",
    "print(\"Shape :\",df1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Correlation of features \n",
    "plt.figure(figsize=(18, 12))\n",
    "sns.heatmap(round(df.corr(), 2), annot=True, vmin=-1, vmax=1, cbar=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check VIF scores\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = house.columns\n",
    "\n",
    "vif_data[\"VIF\"] = np.round([variance_inflation_factor(house.values, i) for i in range(len(house.columns))], 2)\n",
    "vif_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(dataset, threshold):\n",
    "    col_corr = set() # Set of all the names of deleted columns\n",
    "    corr_matrix = dataset.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if (corr_matrix.iloc[i, j] >= threshold) and (corr_matrix.columns[j] not in col_corr):\n",
    "                colname = corr_matrix.columns[i] # getting the name of column\n",
    "                col_corr.add(colname)\n",
    "                if colname in dataset.columns:\n",
    "                    del dataset[colname] # deleting the column from the dataset\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining Tuning class\n",
    "class RandomSearchCV:\n",
    "    # Defining constructor\n",
    "    def __init__(self, model, parameter_dictionary, n_iter):\n",
    "        # Creating class memebers\n",
    "        self.model = model\n",
    "        self.params = parameter_dictionary\n",
    "        self.n_iter = n_iter\n",
    "        self.best_params_ = None\n",
    "        self.best_score_ = 0\n",
    "        self.params_space = []\n",
    "        \n",
    "        # Generating parameters space\n",
    "        keys = list(self.params.keys())\n",
    "        values = list(self.params.values())\n",
    "        for i in values[0]:\n",
    "            for j in values[1]:\n",
    "                temp = list([dict({keys[0]: i}), dict({keys[1]: j})])\n",
    "                self.params_space.append(temp)\n",
    "        \n",
    "        # Selecting n parameters pairs\n",
    "        rand = np.random.randint(0, len(self.params_space), self.n_iter)\n",
    "        self.select_params = [self.params_space[i] for i in rand]\n",
    "        \n",
    "    # Deefining fit function\n",
    "    def fit(self, X_train, y_train, X_test, y_test):\n",
    "        from tqdm import tqdm\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        \n",
    "        # Training model on all selected params\n",
    "        for i in tqdm(self.select_params):\n",
    "            model = self.model\n",
    "            for j in i:\n",
    "                p = list(j.keys())[0]\n",
    "                model.p = list(j.values())[0]\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            score = accuracy_score(y_test, y_pred)\n",
    "            if score > self.best_score_:\n",
    "                self.best_score_ = score\n",
    "                self.best_params_ = i\n",
    "    \n",
    "    # Function to take parameter space\n",
    "    def get_param_space(self):\n",
    "        return self.params_space\n",
    "\n",
    "    # Function to take parameter space\n",
    "    def get_selected_param_space(self):\n",
    "        return self.select_params\n",
    "    \n",
    "\n",
    "# Defining parameters values\n",
    "params = {\n",
    "    \"max_depth\": [4, 6, 3],\n",
    "    \"criteion\": [\"gini\", \"entropy\"]\n",
    "}\n",
    "\n",
    "# Creating and fitting RandomSearch\n",
    "search = RandomSearchCV(DecisionTreeClassifier(), params, 2)\n",
    "search.fit(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Importing evaluation modules\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# check the performance on diffrent regressor\n",
    "models = []\n",
    "models.append(('Ridge', Ridge()))\n",
    "models.append(('LinearRegression', LinearRegression()))\n",
    "models.append(('KNeighborsRegressor', KNeighborsRegressor()))\n",
    "models.append(('Random Forest', RandomForestRegressor()))\n",
    "models.append(('Decision Tree', DecisionTreeRegressor()))\n",
    "\n",
    "train_l = []\n",
    "test_l = []\n",
    "mae_l = []\n",
    "rmse_l = []\n",
    "r2_l = []\n",
    "\n",
    "import time\n",
    "i = 0\n",
    "for name,model in models:\n",
    "    i = i+1\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Fitting model to the Training set\n",
    "    clf = model\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Scores of model\n",
    "    train = model.score(X_train, y_train)\n",
    "    test = model.score(X_test, y_test)\n",
    "    \n",
    "    train_l.append(train)\n",
    "    test_l.append(test)\n",
    "    \n",
    "    # predict values\n",
    "    predictions = clf.predict(X_test)\n",
    "    # RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "    rmse_l.append(rmse)\n",
    "    # MAE\n",
    "    mae = mean_absolute_error(y_test,predictions)\n",
    "    mae_l.append(mae)\n",
    "    # R2 score\n",
    "    r2 = r2_score(y_test,predictions)\n",
    "    r2_l.append(r2)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"+\",\"=\"*100,\"+\")\n",
    "    print('\\033[1m' + f\"\\t\\t\\t{i}-For {name} The Performance result is: \" + '\\033[0m')\n",
    "    print(\"+\",\"=\"*100,\"+\")\n",
    "    print('Root mean squared error (RMSE) : ', rmse)   \n",
    "    print(\"-\"*50)\n",
    "    print('Mean absolute error (MAE) : ', mae)\n",
    "    print(\"-\"*50)\n",
    "    print('Max errors : ', m_errors)\n",
    "    print(\"-\"*50)\n",
    "    print('R2 score : ', r2)\n",
    "    print(\"-\"*50)\n",
    "    print('cross validation accuracy : ', np.mean(scores))\n",
    "    print(\"-\"*50)\n",
    "\n",
    "\n",
    "    \n",
    "    print(\"\\t\\t\\t\\t\\t\\t\\t-----------------------------------------------------------\")\n",
    "    print(f\"\\t\\t\\t\\t\\t\\t\\t Time for detection ({name}) : {round((time.time() - start_time), 3)} seconds...\")\n",
    "    print(\"\\t\\t\\t\\t\\t\\t\\t-----------------------------------------------------------\")\n",
    "    print()\n",
    "    \n",
    "comp = pd.DataFrame({\"Training Score\": train_l, \"Testing Score\": test_l, \"MAE\": mae_l, \"RMSE\": rmse_l, \"R2 Score\": r2_l})\n",
    "comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-eee7f20936f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;31m# Fitting model to the Training set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m     \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;31m# predict values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Importing Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "# Importing Evaluation matrces\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,confusion_matrix,\\\n",
    "classification_report, plot_confusion_matrix\n",
    "\n",
    "# check the performance on diffrent regressor\n",
    "models = []\n",
    "models.append(('Support Vector Classifier', svm.SVC()))\n",
    "models.append(('LogisitcRegression', LogisticRegression()))\n",
    "models.append(('KNeighborsClassifier', KNeighborsClassifier()))\n",
    "models.append(('RandomForestClassifier', RandomForestClassifier()))\n",
    "models.append(('AdaBoostClassifier', AdaBoostClassifier()))\n",
    "models.append(('DecisionTreeClassifier', DecisionTreeClassifier()))\n",
    "\n",
    "\n",
    "# prepare the cross-validation procedure\n",
    "cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "\n",
    "# metrices to store performance\n",
    "acc = []\n",
    "pre = []\n",
    "f1 = []\n",
    "con = []\n",
    "rec = []\n",
    "\n",
    "\n",
    "import time\n",
    "i = 0\n",
    "for name,model in models:\n",
    "    i = i+1\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Fitting model to the Training set\n",
    "    clf = model\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # predict values\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    acc.append(accuracy)\n",
    "    # Precision\n",
    "    precision = precision_score(y_test, y_pred, average=None)\n",
    "    pre.append(precision)\n",
    "    # Recall\n",
    "    recall = recall_score(y_test, y_pred, average=None)\n",
    "    rec.append(recall)\n",
    "    # F1 Score\n",
    "    f1_sco = f1_score(y_test, y_pred, average=None)\n",
    "    f1.append(f1_sco)\n",
    "    # Confusion Matrix\n",
    "    confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "    con.append(confusion_mat)\n",
    "    # Report\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    # evaluate model\n",
    "    scores = cross_val_score(clf, X, y, cv=cv, n_jobs=-1)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"+\",\"=\"*100,\"+\")\n",
    "    print('\\033[1m' + f\"\\t\\t\\t{i}-For {name} The Performance result is: \" + '\\033[0m')\n",
    "    print(\"+\",\"=\"*100,\"+\")\n",
    "    print('Accuracy : ', accuracy)   \n",
    "    print(\"-\"*50)\n",
    "    print('F1 : ', f1_sco)\n",
    "    print(\"-\"*50)\n",
    "    print('Reacll : ', recall)\n",
    "    print(\"-\"*50)\n",
    "    print('Precision : ', precision)\n",
    "    print(\"-\"*50)\n",
    "    print('cross validation accuracy : ', np.mean(scores))\n",
    "    print(\"-\"*50)\n",
    "    print('Confusion Matrix....\\n', confusion_mat)\n",
    "    print(\"-\"*50)\n",
    "    print('Classification Report....\\n', report)\n",
    "    print(\"-\"*50)\n",
    "    print('Plotting Confusion Matrix...\\n')\n",
    "    plot_confusion_matrix(clf, X_test, y_test)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "    print(\"\\t\\t\\t\\t\\t\\t\\t-----------------------------------------------------------\")\n",
    "    print(f\"\\t\\t\\t\\t\\t\\t\\t Time for detection ({name}) : {round((time.time() - start_time), 3)} seconds...\")\n",
    "    print(\"\\t\\t\\t\\t\\t\\t\\t-----------------------------------------------------------\")\n",
    "    print()\n",
    "    \n",
    "pd.DataFrame({\"Model\": dict(models).keys(), \"Accuracy\": acc, \"Precision\": pre, \"Recall\": rec, \"F1_Score\": f1, \"Confusion Matrix\": con})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "class_weights = {0: 1, 1: 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "sns.barplot(x=list(dict(models).keys()), y=acc)\n",
    "plt.title(\"Model's Accuracies\", fontsize=22)\n",
    "plt.xlabel(\"Models\", fontsize=17)\n",
    "plt.ylabel(\"Accuracy\", fontsize=17)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "models.append(('LR', LinearRegression()))\n",
    "models.append(('RFR', RandomForestRegressor()))\n",
    "models.append(('KNN', KNeighborsRegressor()))\n",
    "models.append(('CART', DecisionTreeRegressor()))\n",
    "models.append(('SVM', LinearSVR()))\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=10)\n",
    "    cv_results = cross_val_score(model, X, y, cv=kfold, scoring='neg_mean_absolute_error')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "# boxplot algorithm comparison\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModelSelection(test_data,features,label,dummy_variables_list):\n",
    "    MLA = [\n",
    "    \n",
    "    ensemble.BaggingClassifier(),\n",
    "    ensemble.GradientBoostingClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "           \n",
    "    XGBClassifier(),\n",
    "        \n",
    "    linear_model.LogisticRegressionCV(),\n",
    "    linear_model.SGDClassifier(),\n",
    "            \n",
    "    svm.SVC(probability=True),\n",
    "        \n",
    "    tree.DecisionTreeClassifier(),\n",
    "                \n",
    "    ]\n",
    "    \n",
    "    MLA_columns = ['MLA Name', 'MLA Parameters','MLA Score']\n",
    "    MLA_compare = pd.DataFrame(columns = MLA_columns)\n",
    "    features_with_customer_id=features.copy()\n",
    "    features_with_customer_id.append('uri')\n",
    "    x_train,x_test,y_train,y_test = train_test_split (test_data[features_with_customer_id],test_data[label],test_size=0.3,random_state=0)\n",
    "    print('features used: ',features)\n",
    "    x_train_backup=x_train\n",
    "    x_test_backup=x_test\n",
    "    x_train=x_train[features]\n",
    "    x_test=x_test[features]\n",
    "    #x_train=pd.get_dummies(x_train, columns=dummy_variables_list)\n",
    "    #x_test=pd.get_dummies(x_test, columns=dummy_variables_list)\n",
    "    row_index = 0\n",
    "    MLA_predict = test_data[label]\n",
    "    for alg in MLA:\n",
    "\n",
    "        MLA_name = alg.__class__.__name__\n",
    "        MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n",
    "        MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n",
    "        alg.fit(x_train, y_train)\n",
    "        MLA_predict[MLA_name] = alg.predict(x_test)\n",
    "        MLA_compare.loc[row_index, 'MLA Score']=alg.score(x_test,y_test)\n",
    "        row_index+=1\n",
    "\n",
    "    \n",
    "    MLA_compare.sort_values(by = ['MLA Score'], ascending = False, inplace = True)\n",
    "    return MLA_compare,x_train,x_test,y_train,y_test,x_train_backup,x_test_backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature selection\n",
    "def feature_selection(features,clf,threshold):\n",
    "    important_features=[]\n",
    "    feature_importance=[]\n",
    "    feature_score = pd.DataFrame(columns=['feature','importance_score'])\n",
    "    \n",
    "    for feature in zip(features, clf.feature_importances_):      \n",
    "      feature_score.loc[len(feature_score.index)] = [feature[0],feature[1]]\n",
    "    \n",
    "    feature_importance=feature_score.sort_values(by=['importance_score'],ascending=False).reset_index().head(threshold)\n",
    "    important_features=feature_importance['feature'].to_list()\n",
    "    return important_features,feature_importance,feature_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For K Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find best number of clusters using Elbow method\n",
    "\n",
    "wcss=[]\n",
    "number_clusters = range(1,7)\n",
    "for i in range(1,7):\n",
    "    kmeans = KMeans(i)\n",
    "    kmeans.fit(df)\n",
    "    wcss_iter = kmeans.inertia_\n",
    "    wcss.append(wcss_iter)\n",
    "\n",
    "plt.plot(number_clusters,wcss)\n",
    "plt.title('The Elbow Method', fontsize=18)\n",
    "plt.xlabel('Number of clusters', fontsize=16)\n",
    "plt.ylabel('WCSS', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# In this graph we can see that the elbow is mad at x value 2. That's mean the best number of clusters are 2 for our dataset.\n",
    "# Let's Build Final Model\n",
    "model = KMeans(n_clusters = 3, init = \"k-means++\", random_state = 42)\n",
    "y_kmeans = model.fit_predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "import pickle as pkl\n",
    "\n",
    "filename = 'finalized_model.sav'\n",
    "pkl.dump(random_classifier, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(start_time=None):\n",
    "    if not start_time:\n",
    "        start_time = datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n",
    "        tmin, tsec = divmod(temp_sec, 60)\n",
    "        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
