{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce5a2a87",
   "metadata": {},
   "source": [
    "> # **Dependecies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "76326c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install neattext\n",
    "#!pip install re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f4b20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd               #python package for data analysis\n",
    "import numpy as np                #python package for handling arrays \n",
    "import re                         #python package for dealing with regular expressions\n",
    "import neattext as nt             #python package for some regex and text cleaninig purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56dda4c",
   "metadata": {},
   "source": [
    "> # **Spacy Language Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b4d01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should have python 3.6\n",
    "#!pip install spacy\n",
    "#!pip install scispacy\n",
    "#spacy donwnload en_core_web_lg\n",
    "#spacy download en_ner_bc5cdr_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7726cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy                            #premium python package for NLP\n",
    "nlp=spacy.load('en_core_web_lg')        #importing spacy large language model\n",
    "import scispacy                        #scientific spacy (scispacy) for biomedical uses\n",
    "sci_nlp=spacy.load((\"en_ner_bc5cdr_md\") #importing bc5cdr medium model for detection of diseases and drugd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5498424",
   "metadata": {},
   "source": [
    "> # **Regular Expressions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979d96be",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_passport_regex=r'^[A-Z]{2}\\d{7}[A-Z]$'\n",
    "maldv_passport_regex=r'^[A-Z]\\d{6}$'\n",
    "china_passport_regex=r'^[A-Z]\\d{8,9}$'\n",
    "indonsh_passport_regex=r'^[A-Z]\\d{9}$'\n",
    "pak_passport_regex=r'^[A-Z]{2}\\d{7}[A-Z]{3}$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3dd8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_natid_regex=r'^\\d{12}$'\n",
    "pak_natid_regex=r'^\\d{5}\\d{7}\\d$'\n",
    "sri_natid_regex=r'^\\d{9}[VvXx]$'\n",
    "bangla_natid_regex=r'^\\d{4}\\d{2}\\d{2}\\d{4}\\d{5}$'\n",
    "nepal_natid_regex=r'^\\d{10}\\d{3}$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe194576",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssn_regex = r'^\\d{3}-\\d{2}-\\d{4}$'\n",
    "china_national_id_regex = r'^[A-Z]\\d{17}$'\n",
    "india_national_id_regex = r'^\\d{12}$'\n",
    "japan_national_id_regex = r'^\\d{12}$'\n",
    "pakistan_national_id_regex = r'^\\d{13}$'\n",
    "philippines_national_id_regex = r'^\\d{12}$'\n",
    "ifsc_regex=\"^[A-Z]{4}0[A-Z0-9]{6}$\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6d6382",
   "metadata": {},
   "outputs": [],
   "source": [
    "emails=[]\n",
    "phone_no=[]\n",
    "names=[]\n",
    "drugs=[]\n",
    "diseases=[]\n",
    "national_ids=[]\n",
    "passport_no=[]\n",
    "gpe=[]\n",
    "locations=[]\n",
    "urls=[]\n",
    "zip_codes=[]\n",
    "orgs=[]\n",
    "ip_addresses=[]\n",
    "mac_addresses=[]\n",
    "ssn=[]\n",
    "master_visa_cards=[]\n",
    "ifsc=[]\n",
    "fb_profiles=[]\n",
    "telegram_profiles=[]\n",
    "twitter_profiles=[]\n",
    "linkedin_profiles=[]\n",
    "insta_profiles=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e092a0",
   "metadata": {},
   "source": [
    "> # **Critical PII**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4940fe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_critical_pii(data):\n",
    "    '''\n",
    "    This function extracts critical PII\n",
    "    '''\n",
    "    raw_data=data\n",
    "    emails.extend(re.findall(re.compile(r'[\\w\\.-]+@[\\w\\.-]+', re.UNICODE),raw_data))\n",
    "    phone_no.extend(re.findall(re.compile(r'[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]',re.UNICODE),raw_data))\n",
    "    phone_no=[ph_no for ph_no in phone_no if len(ph_no)<=15]\n",
    "    doc=nlp(data)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_=='PERSON':\n",
    "            names.append(ent.text)\n",
    "    doc=sci_nlp(data)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_=='DISEASE':\n",
    "            dieases.append(ent.text)\n",
    "        else:\n",
    "            drugs.append(ent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31facef",
   "metadata": {},
   "source": [
    "> # **Sensitive PII**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad85488d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sensitive_pii(data):\n",
    "    '''\n",
    "    This function extracts sensitive PII\n",
    "    '''\n",
    "    raw_data=data\n",
    "    for element in data.split('$'):\n",
    "        ssn.extend(re.findall(r'^\\d{3}-\\d{2}-\\d{4}$',element.strip()))\n",
    "        ifsc.extend(re.findall(ifsc_regex,element.strip()))\n",
    "        master_visa_cards.extend(re.findall(re.compile(r'(?:(?:(?:\\d{4}[- ]?){3}\\d{4}|\\d{15,16}))',re.UNICODE),element.strip()))\n",
    "        master_visa_cards.extend(re.findall(re.compile(r'5[1-5]\\d{2}[\\s-]?\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}',re.UNICODE),element.strip()))\n",
    "        passport_no.extend(re.findall(ind_passport_regex,element.strip()))\n",
    "        passport_no.extend(re.findall(maldv_passport_regex,element.strip()))\n",
    "        passport_no.extend(re.findall(china_passport_regex,element.strip()))\n",
    "        passport_no.extend(re.findall(indonsh_passport_regex,element.strip()))\n",
    "        passport_no.extend(re.findall(pak_passport_regex,element.strip()))\n",
    "        national_ids.extend(re.findall(ind_natid_regex,element.strip()))\n",
    "        national_ids.extend(re.findall(pak_natid_regex,element.strip()))\n",
    "        national_ids.extend(re.findall(sri_natid_regex,element.strip()))\n",
    "        national_ids.extend(re.findall(bangla_natid_regex,element.strip()))\n",
    "        national_ids.extend(re.findall(nepal_natid_regex,element.strip()))\n",
    "        urls.extend(nt.extract_urls(raw_data))\n",
    "        for url in urls:\n",
    "            if 'facebook' in url:\n",
    "                fb_profiles.append(url)\n",
    "                del urls[url]\n",
    "            if 't.me' in url:\n",
    "                telegram_profiles.append(url)\n",
    "                del urls[url]\n",
    "            if 'linkedin' in url:\n",
    "                linkedin_profiles.append(url)\n",
    "                del urls[url]\n",
    "            if 'twitter' in url:\n",
    "                twitter_profiles.append(url)\n",
    "                del urls[url]\n",
    "            if 'instagram' in url:\n",
    "                insta_profiles.append(url)\n",
    "                del urls[url]\n",
    "\n",
    "        doc=nlp(raw_data)\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_=='GPE':\n",
    "                gpe.append(ent.text)\n",
    "            if ent.label_=='LOC':\n",
    "                locations.append(ent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd5dae7",
   "metadata": {},
   "source": [
    "> # **Quasi PII**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedf109a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_quasi_pii(data):\n",
    "    '''\n",
    "    This function extracts Quasi PII\n",
    "    '''\n",
    "    for element in data.split('$'):\n",
    "        ip_addresses.extend(re.findall(r\"[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\",element.strip()))\n",
    "        mac_addresses.extend(re.findall(r'^(?:[0-9A-Fa-f]{2}[:-]){5}(?:[0-9A-Fa-f]{2})$',element.strip()))\n",
    "    doc=nlp(data)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_=='ORG':\n",
    "            orgs.append(ent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31215c06",
   "metadata": {},
   "source": [
    "> # **Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564b3439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification():\n",
    "    '''\n",
    "    This function collect all the extracted data and classify into their respective categories\n",
    "    '''\n",
    "    critical_pii={'Names':names,'Emails':emails,'Phone No':phone_no,'Diseases':diseases,'Drugs':drugs}\n",
    "    critical_df=pd.DataFrame(critical_pii)\n",
    "    sensitive_pii={'SSN':ssn,'Master Visa Cards':master_visa_cards,'IFSC':ifsc,\n",
    "                   'National ID':national_ids,'Passport No':passport_no,'Telegram Profiles':telegram_profiles,\n",
    "                  'Twitter Profiles':twitter_profiles,'LinkedIn Profiles':linkedin_profiles,'Instagram Profiles':insta_profiles\n",
    "                  ,'URLs':urls,'Geographic Locations':gpe,'Locations':locations}\n",
    "    sensitive_df=pd.DataFrame(sensitive_pii)\n",
    "    quasi_pii={'Organizations':orgs,'IP Addresses':ip_addresses,'MAC Addresses':mac_addresses}\n",
    "    quasi=pd.DataFrame(quasi_pii)\n",
    "    return critical_df,sensitive_df,quasi_pii"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b2b05f",
   "metadata": {},
   "source": [
    "> # **Extract PII**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f2db638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_PII(df):\n",
    "    '''\n",
    "    This function receives a dataframe of csv file and format the dataframe and pass to each \n",
    "    of critical,sensitive and quasi pii extraction functions.\n",
    "    '''\n",
    "    all_data=df.values\n",
    "    for value in all_data:\n",
    "        data=' $ '.join(value)\n",
    "        extract_critical_pii(data)      #passing data to critical pii extraction function\n",
    "        extract_sensitive_pii(data)     #passing data to sensitive pii extraction function\n",
    "        extract_quasi_pii(data)         #passing data to quasi pii extraction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b15fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('file_name.csv')    #reading the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ff2fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_PII(df)                   #calling extract_PII function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db13515",
   "metadata": {},
   "source": [
    "> # **Saving the Extacted Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd85ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "critical_df.to_csv('critical_pii.csv')\n",
    "sensitive_df.to_csv('sensitive_pii.csv')\n",
    "quasi_pii,to_csv('quasi_pii.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
