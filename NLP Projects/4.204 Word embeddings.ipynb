{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An alternative approach\n",
    "Can we define words by the company they keep?  \n",
    "\"If A and B have almost the identical environments we can say that they are synonyms\" (Zelig Harris, 1954)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector representations\n",
    "One-hot encodings are long and sparse  \n",
    "Alternative: **dense vectors**  \n",
    "short (length 50-1000) + dense (most elements are non-zero)\n",
    "### Benefits\n",
    "1. Easier to use in ML  \n",
    "2. Offer better generalization capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train your own embeddings\n",
    "Instead of counting terms, we train a classifier on a prediction task: 'does A occur near B'?  \n",
    "The learned classifier weights become our embeddings   \n",
    "We can create our own word embeddings using gensim (an open-source library for unsupervised topic modeling and NLP)  \n",
    "and train it on the Brown corpus  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\zee tech\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "[nltk_data] Downloading package brown to C:\\Users\\Zee\n",
      "[nltk_data]     Tech\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up libraries & data\n",
    "import gensim\n",
    "from nltk.corpus import brown\n",
    "import nltk\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\zee tech\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\gensim\\models\\base_any2vec.py:742: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "model = gensim.models.Word2Vec(brown.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a copy, for later re-use\n",
    "model.save('brown.embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can load models on demand\n",
    "brown_model = gensim.models.Word2Vec.load('brown.embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Word2VecKeyedVectors' object has no attribute 'index_to_key'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-0ea6648af86f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# how many words?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbrown_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_to_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Word2VecKeyedVectors' object has no attribute 'index_to_key'"
     ]
    }
   ],
   "source": [
    "# how many words?\n",
    "len(brown_model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many dimensions?\n",
    "brown_model.wv['university']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate similarity between terms\n",
    "brown_model.wv.similarity('university','school')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('membership', 0.9543715119361877),\n",
       " ('profession', 0.9533947110176086),\n",
       " ('neighborhood', 0.9528996348381042),\n",
       " ('congregation', 0.951430082321167),\n",
       " ('selection', 0.9481860399246216)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find similar terms\n",
    "brown_model.wv.most_similar('university', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('marble', 0.9657975435256958),\n",
       " ('pension', 0.9649026989936829),\n",
       " ('frankfurters', 0.9639797806739807),\n",
       " ('herd', 0.9625921249389648),\n",
       " ('towel', 0.9624603986740112)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_model.wv.most_similar('lemon', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nation', 0.9277928471565247),\n",
       " ('policy', 0.9251207709312439),\n",
       " ('power', 0.9233179092407227),\n",
       " ('Christian', 0.9233003258705139),\n",
       " ('education', 0.9203839302062988)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_model.wv.most_similar('government', topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Use pre-trained embeddings\n",
    "We can load pre-built embeddings, e.g. a sample from a model trained on 100 billion words from the Google News Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package word2vec_sample to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping models/word2vec_sample.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('word2vec_sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a pre-build model\n",
    "from nltk.data import find\n",
    "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "news_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43981"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many terms?\n",
    "len(news_model.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many dimensions?\n",
    "len(news_model['university'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('universities', 0.7003918290138245),\n",
       " ('faculty', 0.6780906915664673),\n",
       " ('undergraduate', 0.6587096452713013),\n",
       " ('campus', 0.6434987783432007),\n",
       " ('college', 0.638526976108551)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# are they any better?\n",
    "news_model.most_similar(positive=['university'], topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lemons', 0.646256148815155),\n",
       " ('apricot', 0.6199417114257812),\n",
       " ('avocado', 0.5922889113426208),\n",
       " ('fennel', 0.5873182415962219),\n",
       " ('coriander', 0.5828487277030945)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.most_similar(positive=['lemon'], topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Government', 0.7132059335708618),\n",
       " ('governments', 0.6521531939506531),\n",
       " ('administration', 0.5462369322776794),\n",
       " ('legislature', 0.5307289361953735),\n",
       " ('parliament', 0.5268454551696777)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.most_similar(positive=['government'], topn = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Perform vector algebra\n",
    "We can use embeddings to perform verbal reasoning, e.g. A is to B as C is to...  \n",
    "e.g. 'man is to king as woman is to...'  \n",
    "vec(“king”) - vec(“man”) + vec(“woman”) =~ vec(“queen”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118193507194519),\n",
       " ('monarch', 0.6189674139022827),\n",
       " ('princess', 0.5902430415153503),\n",
       " ('prince', 0.5377321243286133),\n",
       " ('kings', 0.5236843228340149)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.most_similar(positive=['woman','king'], negative=['man'], topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118193507194519),\n",
       " ('monarch', 0.6189674139022827),\n",
       " ('princess', 0.5902430415153503),\n",
       " ('prince', 0.5377321243286133),\n",
       " ('kings', 0.5236843228340149)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.most_similar(positive=['king', 'woman'], negative=['man'], topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('France', 0.7884091734886169),\n",
       " ('Belgium', 0.6197876930236816),\n",
       " ('Spain', 0.5664774179458618),\n",
       " ('Italy', 0.5654898881912231),\n",
       " ('Switzerland', 0.560969352722168)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encyclopaedic knowledge\n",
    "news_model.most_similar(positive=['Paris','Germany'], negative=['Berlin'], topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('is', 0.6774996519088745),\n",
       " ('was', 0.5710028409957886),\n",
       " ('remains', 0.47552669048309326),\n",
       " ('been', 0.4538103938102722),\n",
       " ('being', 0.4456518888473511)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# syntactic patterns (verbs)\n",
    "news_model.most_similar(positive=['has','be'], negative=['have'], topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('shortest', 0.5145131945610046),\n",
       " ('steepest', 0.42448338866233826),\n",
       " ('first', 0.40251171588897705),\n",
       " ('flattest', 0.40171927213668823),\n",
       " ('consecutive', 0.39518705010414124)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# syntactic patterns (adjectives)\n",
    "news_model.most_similar(positive=['longest','short'], negative=['long'], topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('purple', 0.5252774953842163),\n",
       " ('tulips', 0.4938238859176636),\n",
       " ('brown', 0.4907749593257904),\n",
       " ('pink', 0.4860530197620392),\n",
       " ('maroon', 0.48056456446647644)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# more encyclopaedic knowledge\n",
    "news_model.most_similar(positive=['blue','tulip'], negative=['sky'], topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('her', 0.804938554763794),\n",
       " ('herself', 0.6881042718887329),\n",
       " ('me', 0.5886672139167786),\n",
       " ('She', 0.5803765058517456),\n",
       " ('woman', 0.5470799207687378)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# syntactic knowledge (pronouns)\n",
    "news_model.most_similar(positive=['him','she'], negative=['he'], topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('short', 0.4077243208885193),\n",
       " ('longer', 0.3670078217983246),\n",
       " ('lengthy', 0.36229580640792847),\n",
       " ('Long', 0.36003556847572327),\n",
       " ('continuous', 0.34982720017433167)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lexical know\n",
    "news_model.most_similar(positive=['light','long'], negative=['dark'], topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cereal'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the odd one out\n",
    "news_model.doesnt_match('breakfast cereal dinner lunch'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz questions + homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('have', 0.7298133969306946),\n",
       " ('Had', 0.5585500597953796),\n",
       " ('subsequently', 0.54400634765625),\n",
       " ('had', 0.5378279685974121),\n",
       " ('recently', 0.5339969992637634)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.most_similar(positive=['be','has'], negative=['is'], topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('membership', 0.9543715119361877),\n",
       " ('profession', 0.9533947110176086),\n",
       " ('neighborhood', 0.9528996348381042),\n",
       " ('congregation', 0.951430082321167),\n",
       " ('selection', 0.9481860399246216)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_model.wv.most_similar('university', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('universities', 0.7003918290138245),\n",
       " ('faculty', 0.6780906915664673),\n",
       " ('undergraduate', 0.6587096452713013),\n",
       " ('campus', 0.6434987783432007),\n",
       " ('college', 0.638526976108551)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.most_similar('university', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('university', 0.9235467314720154),\n",
       " ('treatment', 0.9229241609573364),\n",
       " ('selection', 0.921475887298584),\n",
       " ('persons', 0.9177237153053284),\n",
       " ('generations', 0.9159466624259949)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_model.wv.most_similar('college', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('colleges', 0.6560819149017334),\n",
       " ('university', 0.6385270357131958),\n",
       " ('school', 0.6081898808479309),\n",
       " ('collegiate', 0.6081600189208984),\n",
       " ('undergraduate', 0.5866836905479431)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.most_similar('college', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.054503173"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.similarity('university','turtle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5080746"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.similarity('university', 'school')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13740103"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.similarity('university', 'factory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13598306"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.similarity('university', 'supermarket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.054503173"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.similarity('university', 'turtle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_model.similarity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
