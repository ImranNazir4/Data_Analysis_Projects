{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An alternative approach\n",
    "Can we define words by the company they keep?  \n",
    "\"If A and B have almost the identical environments we can say that they are synonyms\" (Zelig Harris, 1954)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector representations\n",
    "One-hot encodings are long and sparse  \n",
    "Alternative: **dense vectors**  \n",
    "short (length 50-1000) + dense (most elements are non-zero)\n",
    "### Benefits\n",
    "1. Easier to use in ML  \n",
    "2. Offer better generalization capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train your own embeddings\n",
    "Instead of counting terms, we train a classifier on a prediction task: 'does A occur near B'?  \n",
    "The learned classifier weights become our embeddings   \n",
    "We can create our own word embeddings using gensim (an open-source library for unsupervised topic modeling and NLP)  \n",
    "and train it on the Brown corpus  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up libraries & data\n",
    "import gensim\n",
    "from nltk.corpus import brown\n",
    "import nltk\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "model = gensim.models.Word2Vec(brown.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a copy, for later re-use\n",
    "model.save('brown.embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can load models on demand\n",
    "brown_model = gensim.models.Word2Vec.load('brown.embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15173"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many words?\n",
    "len(brown_model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.11187179,  0.2567835 ,  0.20067121,  0.11146956, -0.06449822,\n",
       "       -0.3327126 ,  0.20342724,  0.34695333, -0.30680928, -0.28263548,\n",
       "        0.16839471, -0.22766627,  0.19039585,  0.16696046,  0.24309714,\n",
       "       -0.16298178,  0.26914582, -0.1446194 , -0.5225709 , -0.5176164 ,\n",
       "        0.28516993, -0.11437444,  0.48385173,  0.07618304, -0.05912062,\n",
       "       -0.13288675, -0.23004812,  0.01751912, -0.20995356,  0.23441617,\n",
       "        0.2166875 , -0.06708905,  0.2807649 , -0.37060753, -0.17715661,\n",
       "        0.05822577, -0.18903217, -0.05345417, -0.3441689 , -0.05966146,\n",
       "        0.01254072, -0.2682951 ,  0.17967753,  0.11451203,  0.20362483,\n",
       "       -0.00841677, -0.01906935, -0.02458966,  0.09951635,  0.31048715,\n",
       "        0.01646458, -0.28577495, -0.2510325 , -0.18965784, -0.10745484,\n",
       "       -0.22962518,  0.17678723,  0.0517607 , -0.06233544, -0.07932491,\n",
       "        0.02962325,  0.20045765, -0.03775026, -0.16829008, -0.18935414,\n",
       "        0.45213303,  0.02303696,  0.28415978, -0.2634968 ,  0.3702597 ,\n",
       "        0.2048647 ,  0.14639002,  0.21681611, -0.00263781,  0.334834  ,\n",
       "        0.08963317,  0.10742171,  0.04744329,  0.02959783, -0.12967984,\n",
       "       -0.11853542,  0.01166017, -0.21744977,  0.07354469, -0.18600895,\n",
       "       -0.01746792,  0.19002153,  0.01776374,  0.16582842,  0.07587875,\n",
       "        0.42543125, -0.1520814 , -0.12045213,  0.11298627,  0.3235232 ,\n",
       "        0.11950471,  0.1672888 , -0.36427152, -0.15208828,  0.05163104],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many dimensions?\n",
    "brown_model.wv['university']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8155095"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate similarity between terms\n",
    "brown_model.wv.similarity('university','school')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('membership', 0.9543715119361877),\n",
       " ('profession', 0.9533947110176086),\n",
       " ('neighborhood', 0.9528996348381042),\n",
       " ('congregation', 0.951430082321167),\n",
       " ('selection', 0.9481860399246216)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find similar terms\n",
    "brown_model.wv.most_similar('university', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('marble', 0.9657975435256958),\n",
       " ('pension', 0.9649026989936829),\n",
       " ('frankfurters', 0.9639797806739807),\n",
       " ('herd', 0.9625921249389648),\n",
       " ('towel', 0.9624603986740112)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_model.wv.most_similar('lemon', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nation', 0.9277928471565247),\n",
       " ('policy', 0.9251207709312439),\n",
       " ('power', 0.9233179092407227),\n",
       " ('Christian', 0.9233003258705139),\n",
       " ('education', 0.9203839302062988)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_model.wv.most_similar('government', topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Use pre-trained embeddings\n",
    "We can load pre-built embeddings, e.g. a sample from a model trained on 100 billion words from the Google News Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package word2vec_sample to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping models/word2vec_sample.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('word2vec_sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a pre-build model\n",
    "from nltk.data import find\n",
    "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "news_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43981"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many terms?\n",
    "len(news_model.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many dimensions?\n",
    "len(news_model['university'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('universities', 0.7003918290138245),\n",
       " ('faculty', 0.6780906915664673),\n",
       " ('undergraduate', 0.6587096452713013),\n",
       " ('campus', 0.6434987783432007),\n",
       " ('college', 0.638526976108551)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# are they any better?\n",
    "news_model.most_similar(positive=['university'], topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lemons', 0.646256148815155),\n",
       " ('apricot', 0.6199417114257812),\n",
       " ('avocado', 0.5922889113426208),\n",
       " ('fennel', 0.5873182415962219),\n",
       " ('coriander', 0.5828487277030945)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.most_similar(positive=['lemon'], topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Government', 0.7132059335708618),\n",
       " ('governments', 0.6521531939506531),\n",
       " ('administration', 0.5462369322776794),\n",
       " ('legislature', 0.5307289361953735),\n",
       " ('parliament', 0.5268454551696777)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.most_similar(positive=['government'], topn = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Perform vector algebra\n",
    "We can use embeddings to perform verbal reasoning, e.g. A is to B as C is to...  \n",
    "e.g. 'man is to king as woman is to...'  \n",
    "vec(“king”) - vec(“man”) + vec(“woman”) =~ vec(“queen”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118193507194519),\n",
       " ('monarch', 0.6189674139022827),\n",
       " ('princess', 0.5902430415153503),\n",
       " ('prince', 0.5377321243286133),\n",
       " ('kings', 0.5236843228340149)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.most_similar(positive=['woman','king'], negative=['man'], topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118193507194519),\n",
       " ('monarch', 0.6189674139022827),\n",
       " ('princess', 0.5902430415153503),\n",
       " ('prince', 0.5377321243286133),\n",
       " ('kings', 0.5236843228340149)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.most_similar(positive=['king', 'woman'], negative=['man'], topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('France', 0.7884091734886169),\n",
       " ('Belgium', 0.6197876930236816),\n",
       " ('Spain', 0.5664774179458618),\n",
       " ('Italy', 0.5654898881912231),\n",
       " ('Switzerland', 0.560969352722168)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encyclopaedic knowledge\n",
    "news_model.most_similar(positive=['Paris','Germany'], negative=['Berlin'], topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('is', 0.6774996519088745),\n",
       " ('was', 0.5710028409957886),\n",
       " ('remains', 0.47552669048309326),\n",
       " ('been', 0.4538103938102722),\n",
       " ('being', 0.4456518888473511)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# syntactic patterns (verbs)\n",
    "news_model.most_similar(positive=['has','be'], negative=['have'], topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('shortest', 0.5145131945610046),\n",
       " ('steepest', 0.42448338866233826),\n",
       " ('first', 0.40251171588897705),\n",
       " ('flattest', 0.40171927213668823),\n",
       " ('consecutive', 0.39518705010414124)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# syntactic patterns (adjectives)\n",
    "news_model.most_similar(positive=['longest','short'], negative=['long'], topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('purple', 0.5252774953842163),\n",
       " ('tulips', 0.4938238859176636),\n",
       " ('brown', 0.4907749593257904),\n",
       " ('pink', 0.4860530197620392),\n",
       " ('maroon', 0.48056456446647644)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# more encyclopaedic knowledge\n",
    "news_model.most_similar(positive=['blue','tulip'], negative=['sky'], topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('her', 0.804938554763794),\n",
       " ('herself', 0.6881042718887329),\n",
       " ('me', 0.5886672139167786),\n",
       " ('She', 0.5803765058517456),\n",
       " ('woman', 0.5470799207687378)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# syntactic knowledge (pronouns)\n",
    "news_model.most_similar(positive=['him','she'], negative=['he'], topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('short', 0.4077243208885193),\n",
       " ('longer', 0.3670078217983246),\n",
       " ('lengthy', 0.36229580640792847),\n",
       " ('Long', 0.36003556847572327),\n",
       " ('continuous', 0.34982720017433167)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lexical know\n",
    "news_model.most_similar(positive=['light','long'], negative=['dark'], topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cereal'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the odd one out\n",
    "news_model.doesnt_match('breakfast cereal dinner lunch'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz questions + homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('have', 0.7298133969306946),\n",
       " ('Had', 0.5585500597953796),\n",
       " ('subsequently', 0.54400634765625),\n",
       " ('had', 0.5378279685974121),\n",
       " ('recently', 0.5339969992637634)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.most_similar(positive=['be','has'], negative=['is'], topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('membership', 0.9543715119361877),\n",
       " ('profession', 0.9533947110176086),\n",
       " ('neighborhood', 0.9528996348381042),\n",
       " ('congregation', 0.951430082321167),\n",
       " ('selection', 0.9481860399246216)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_model.wv.most_similar('university', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('universities', 0.7003918290138245),\n",
       " ('faculty', 0.6780906915664673),\n",
       " ('undergraduate', 0.6587096452713013),\n",
       " ('campus', 0.6434987783432007),\n",
       " ('college', 0.638526976108551)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.most_similar('university', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('university', 0.9235467314720154),\n",
       " ('treatment', 0.9229241609573364),\n",
       " ('selection', 0.921475887298584),\n",
       " ('persons', 0.9177237153053284),\n",
       " ('generations', 0.9159466624259949)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_model.wv.most_similar('college', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('colleges', 0.6560819149017334),\n",
       " ('university', 0.6385270357131958),\n",
       " ('school', 0.6081898808479309),\n",
       " ('collegiate', 0.6081600189208984),\n",
       " ('undergraduate', 0.5866836905479431)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.most_similar('college', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.054503173"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.similarity('university','turtle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5080746"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.similarity('university', 'school')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13740103"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.similarity('university', 'factory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13598306"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.similarity('university', 'supermarket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.054503173"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.similarity('university', 'turtle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_model.similarity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
