{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oo-YtBCjIMSz"
   },
   "source": [
    "# **Forecasting Paragorn Corps Sale Final Project** üíé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Kex4ijuIRr_"
   },
   "source": [
    "## 1. Introduction üëã\n",
    "\n",
    "*   Nama: Karel Gideon Anugrah Hutajulu\n",
    "*   Batch: FTDS HCK-004\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFs1VUNgIaYF"
   },
   "source": [
    "## 2. Deployment Link üîó"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "my7iPj5OQRWd"
   },
   "source": [
    "**Hugging Face Deployment Link:** https://huggingface.co/spaces/karelgideon/talent-fair-h8-karel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tzFNKQKBIen9"
   },
   "source": [
    "## 3. Working Area ‚õë"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqzIPtoOTGDm"
   },
   "source": [
    "### A. IMPORT LIBRARIES üíΩ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qjfRHNqcTCV1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "#Time Series Model libraries\n",
    "import xgboost as xgb\n",
    "from statsmodels.tsa.api import ExponentialSmoothing\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "from sklearn.metrics import mean_absolute_percentage_error as mape\n",
    "\n",
    "#Attach images to the google notebook\n",
    "from IPython.display import Image\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvprTXmpTSZK"
   },
   "source": [
    "### B. DATA LOADING üíΩ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WtjdSUy5ga94",
    "outputId": "9d84985c-f14d-47f6-da21-83d65b936984"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/drive'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gkYy4RzOtDm1"
   },
   "source": [
    "First of all i upload the dataset and display the head and tail to get a deeper understanding of the data im working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "ocOQ4KkkTRMm",
    "outputId": "73db23b3-f658-4256-ad8b-8b70c67b3869"
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('/content/drive/MyDrive/sample_dataset_timeseries_noarea.csv')\n",
    "\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "azOqp06OTV3S",
    "outputId": "7b3b6205-58af-411a-a869-5d722abbc39f"
   },
   "outputs": [],
   "source": [
    "df1.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UOMo7bLHtXKw"
   },
   "source": [
    "**Observation:**\n",
    "\n",
    "Looks like the data im working with is grouped by perweek. Since product_item is a free variable we will ignore this column.\n",
    "\n",
    "The level of aggregation or granularity (weekly) may result in the loss of fine-grained patterns, making it harder to identify seasonality or trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lnenpN1JTYqu",
    "outputId": "efa8996a-5c3d-4661-b541-95e1a98370be"
   },
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WRPxFrclvbWu",
    "outputId": "48eed978-74c1-47c6-dd78-2afbd97f3519"
   },
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTpWuJGjh48f"
   },
   "source": [
    "### C. DATA CLEANING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NPY_v9e0qa5"
   },
   "source": [
    "**1. Checking duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7yCoHHwRh6lw",
    "outputId": "2eb47071-fbf3-4f55-9f19-dd69e17edc89"
   },
   "outputs": [],
   "source": [
    "df1.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qT6GLxczROsV"
   },
   "source": [
    "**Observation:**\n",
    "There are no duplicates in this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2Ha0JYb0rka"
   },
   "source": [
    "**2. Checking missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xe6GjTw6uIs5",
    "outputId": "d7261d76-b9f0-4011-e487-5055e64afb3c"
   },
   "outputs": [],
   "source": [
    "df1.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SjpJv_yURTBB"
   },
   "source": [
    "**Observation:**\n",
    "There are 2 missing values from the `product_item` column. Lets see what the 2 missing row looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "7puQxQ4NnyXd",
    "outputId": "49199dd8-3a3d-47a6-e18f-4523b0036dd6"
   },
   "outputs": [],
   "source": [
    "# Filter rows with missing values in any column\n",
    "result = df1[df1.isna().any(axis=1)]\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgefWDSKRbr7"
   },
   "source": [
    "**Observation:**\n",
    "As we can see, the 2 missing rows have low quantities. Since the quantity is low and the product_item is a free variable, we will not remove it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWlM6BOMuYt1"
   },
   "source": [
    "### D. Data Visualization and EDA üéÅ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ja5OrZA1u03b"
   },
   "source": [
    "**1. Understanding the date columns**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iRayieEBumx8"
   },
   "source": [
    "To begin i want to check the datatypes of the data. During EDA it is easier to work with time that is in the datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ft3a19kMuv0e",
    "outputId": "f2423d62-508c-4825-8004-2ec594775cc8"
   },
   "outputs": [],
   "source": [
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLXT4fgbvNPK"
   },
   "source": [
    "`week_start_date` and `week_end_date` are object columns. We will convert them to date time columns to make it easier for us to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2geAQW8_21om"
   },
   "outputs": [],
   "source": [
    "# Convert \"week_start_date\" and \"week_end_date\" columns to datetime objects\n",
    "df1['week_start_date'] = pd.to_datetime(df1['week_start_date'])\n",
    "df1['week_end_date'] = pd.to_datetime(df1['week_end_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOKcfjG5u4Uf"
   },
   "source": [
    "Next I want to make sure that every week is consistent for all the data. I am expecting the result of `.mean()` to have a whole number value and the result of the value counts to have the same number as the df1 shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9e30FoNS3FeH",
    "outputId": "7ada703f-01e3-4fe3-b58a-410d9b6c940c"
   },
   "outputs": [],
   "source": [
    "# Calculate the day difference between week_start_date and week_end_date and calculate the average\n",
    "avg_day_difference = (df1['week_end_date'] - df1['week_start_date']).dt.days.mean()\n",
    "\n",
    "print(\"Average day difference between week start and week end:\", avg_day_difference, \"days\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nlmaRbRdoFZ9",
    "outputId": "9012ce46-c75e-4017-a32b-09bd7edbcd26"
   },
   "outputs": [],
   "source": [
    "(df1['week_end_date'] - df1['week_start_date']).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnFl8QclvFTf"
   },
   "source": [
    "**Observation:** Looks like every row is consisten in representing a week. The average shows 6 days because it represent 6 nights and 7 days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJihBDzRvedf"
   },
   "source": [
    "Lastly I want to see if the data is ordered in order of time. I will compare the oldest and newest data with the dataframe when displaying the `.head` and `.tail`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rwx3ybvg4FZ1",
    "outputId": "a9b3b449-cbc6-4814-ddc5-a6e70daf1593"
   },
   "outputs": [],
   "source": [
    "# Find the lowest value of 'week_start_date'\n",
    "lowest_start_date = df1['week_start_date'].min()\n",
    "\n",
    "# Find the latest value of 'week_end_date'\n",
    "latest_end_date = df1['week_end_date'].max()\n",
    "\n",
    "print(\"Lowest 'week_start_date': \", lowest_start_date)\n",
    "print(\"Latest 'week_end_date': \", latest_end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCxy7G1VvzCv"
   },
   "source": [
    "**Observation:** As we can see the dates match and we can safely assume that the dataframe is in order from oldest to newest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B4Qd1mBjvbzo"
   },
   "source": [
    "**2. Understanding the product_id column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4xRTIcaGl1EZ",
    "outputId": "065711e8-2716-4a9e-9077-8dc641c035d5"
   },
   "outputs": [],
   "source": [
    "# Print count of unique values in \"product_item\" column\n",
    "unique_product_items = df1[\"product_item\"].nunique()\n",
    "print(\"Count of unique product items:\", unique_product_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txGd7HsSv29N"
   },
   "source": [
    "There are a lot of different product variations `2309` products. Since the data has `10,000+` rows of data, we know that there are repeating orders of the products.\n",
    "\n",
    "It is possible that the `different product types could have an impact on the total sales in the next week`. So, including contribution of some of them could be beneficial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "hqokaJrYo08X",
    "outputId": "68ae6ce6-7519-4564-e566-3fa77c28e871"
   },
   "outputs": [],
   "source": [
    "plt.plot(df1['product_item'].value_counts().values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yh4Esk9y0UJj"
   },
   "source": [
    "**Observation:** We have a `total of 67 week`s. The above plot shows that there are `around 300-400 product types that are ordered in every week`. So, sale quantity of these products can be included as features for prediction on total sales quantity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWY-4e-eUCIr"
   },
   "source": [
    "Next i want to visualize the distribution of quantity between different products using a pie chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "w8-52HA9Ajoz",
    "outputId": "04002a44-13d2-4e46-f17d-908e888e5d7c"
   },
   "outputs": [],
   "source": [
    "# Groupby product_item and sum the quantity\n",
    "grouped_df = df1.groupby('product_item').sum()\n",
    "grouped_df = grouped_df.sort_values(by='quantity', ascending=False)\n",
    "\n",
    "# Get the top 10 products\n",
    "top_5_df = grouped_df.head(5)\n",
    "\n",
    "# Get the sum of quantity for products not in the top 10\n",
    "others_quantity = grouped_df[~grouped_df.index.isin(top_5_df.index)]['quantity'].sum()\n",
    "\n",
    "# Create a new dataframe for the pie chart\n",
    "pie_df = top_5_df.copy()\n",
    "pie_df.loc['Others'] = others_quantity\n",
    "\n",
    "# Plot a pie chart\n",
    "plt.pie(pie_df['quantity'], labels=pie_df.index, autopct='%1.1f%%')\n",
    "plt.title('Top 10 Highest Quantity Products (Including Others)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R80q6sAMUJM7"
   },
   "source": [
    "**Observation:** As we can see most of the products are sold in low quantities. The top 10 selling products only makes up for less than 15% of the total sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cBUrMDFCUbOH"
   },
   "source": [
    "Lets see how the sales differentiate from order to order, are most of the orders from the same size (big batches of order) or from different sizes. Ex: Some small batch, some big batch. \n",
    "\n",
    "We will do this by plotting a boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "TnVC3Vg6Al_B",
    "outputId": "752b2b30-1ff3-4960-cbaa-3a488bedc712"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Groupby product_item and sum the quantity\n",
    "grouped_df = df1.groupby('product_item').sum()\n",
    "grouped_df = grouped_df.sort_values(by='quantity', ascending=False)\n",
    "\n",
    "# Get the top 10 products\n",
    "top_10_df = grouped_df.head(10)\n",
    "\n",
    "# Filter the original dataframe to include only the top 10 products\n",
    "df_top_10 = df1[df1['product_item'].isin(top_10_df.index)]\n",
    "\n",
    "# Create a box plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot([df_top_10[df_top_10['product_item'] == product]['quantity'] for product in top_10_df.index],\n",
    "            labels=top_10_df.index)\n",
    "plt.title('Box Plot of Quantity for Top 10 Products')\n",
    "plt.xlabel('Product Item')\n",
    "plt.ylabel('Quantity')\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Qap-RMIUoRq"
   },
   "source": [
    "**Observation:** As we can see for the top 10 products. Most of the products are `varied in order sizes`. Especially for the biggest order. It is` hard to identify what type of outlier` this is because of the `lack of context for the data.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SvjQPxZBU8BA"
   },
   "source": [
    "Lastly, I want to see the quantity distribution for all products in the 67 weeks provided by the data.  As we can see, the average quantity of sales each week is greatly influenced by these huge batch of orders.\n",
    "\n",
    "This happens for almost all the weeks `except for Week 21 of 2022`. During that week there are no large orders. This can cause the time series graph to significantly fall down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "esy_7xwcC8Xm",
    "outputId": "403f89bf-7dd2-4885-e4dd-0864b999d9e8"
   },
   "outputs": [],
   "source": [
    "# Set figure size\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Plot boxplot for each week_number\n",
    "sns.boxplot(x='week_number', y='quantity', data=df1)\n",
    "plt.xlabel('Week Number')\n",
    "plt.xticks(rotation=-45, ha='right', fontsize=10)  # Rotate x-axis labels by 45 degrees, align to right, and set font size\n",
    "plt.ylabel('Quantity')\n",
    "plt.title('Boxplot of Quantity by Week Number')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E82fVySJwKtC"
   },
   "source": [
    "### E. Data Preprocessing üç≥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXUKOMVm4Tup"
   },
   "source": [
    "**1. Aggregating the data on a weekly basis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_VmF6wRwlPu"
   },
   "source": [
    "I start by making sure that the week_number and week_start/end_date matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uUqJTjNo5Mn9",
    "outputId": "660cd13e-0170-4cbb-d436-45f34203fd2b"
   },
   "outputs": [],
   "source": [
    "df1['week_number'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "16JhE10a5Mkz",
    "outputId": "185e61e0-cbd2-4615-dd2c-e66409b98889"
   },
   "outputs": [],
   "source": [
    "df1['week_start_date'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VDR859Jb5Mfq",
    "outputId": "b4ee1473-9f93-4ed7-990b-8c48b325ddbc"
   },
   "outputs": [],
   "source": [
    "df1['week_end_date'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1bQcprkrwu_g"
   },
   "source": [
    "Then i create a new dataframe named `df2` that groups by the weeknumber and  use an aggregation function that is applied to calculates the sum of the numeric columns for each quantity per week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZkCovW1y4LTr"
   },
   "outputs": [],
   "source": [
    "df2 = df1.groupby('week_start_date').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "dgMqYyX05rYR",
    "outputId": "ad65b8c4-c150-4a57-8575-2ebe463e3b9d"
   },
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBhKW65bxJvu"
   },
   "source": [
    "Aggregating the data narrows down our data into only 67 rows, meaning there are 67 weeks of data for us to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "NJd6kwbb40z8",
    "outputId": "c34f0b72-30b3-40f1-81ae-52913eb1061a"
   },
   "outputs": [],
   "source": [
    "# Plot the time series data with index on y-axis\n",
    "df2.plot(legend=None)\n",
    "plt.ylabel('Quantity')\n",
    "plt.xlabel('Date')\n",
    "plt.title('Time Series Analysis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "aV9eJOaQrfvr",
    "outputId": "96c5aeaa-7cba-40b3-d412-d4606c5d6da2"
   },
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmH_-9f-2d58"
   },
   "source": [
    "Setting frequency of dataframe to weekly to visualize ACF plots and seasonal decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yNrm0FJupROr"
   },
   "outputs": [],
   "source": [
    "df2.index.freq = 'W-MON'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "CYh-0BM1suTi",
    "outputId": "34933ff9-c529-4168-cbd9-2a9698776edb"
   },
   "outputs": [],
   "source": [
    "title = 'Autocorrelation: Sales quantity'\n",
    "lags = 40\n",
    "plot_acf(df2['quantity'],title=title,lags=lags);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rets9Z2vuGQy"
   },
   "source": [
    "**Observation:** ACF plots shows that there is almost no relation of current quantity sales with previous quantity sales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "ojDcPpSKqK-L",
    "outputId": "49fb8459-cf40-4640-ce54-a53d6a76f25b"
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "result = seasonal_decompose(df2['quantity'].fillna(df2['quantity'].mean()), model='mul', period=3)\n",
    "result.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yT_2Qo7euUBE"
   },
   "source": [
    "**Observation:**  In the seasonal decomposition, it can be seen that the `seasonality component is too small to be considered`. There is also `a little  indication trend.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pxFq-D6uL987"
   },
   "source": [
    "### F. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NkdSXh38MA-j"
   },
   "source": [
    "**1. Feature Creation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fcg9PenaPuXK"
   },
   "source": [
    "I did not add `day` values for feature creation because our data has been grouped by the week and adding day data will miss intepret. The following features will be created in the dataset:\n",
    "1. Product wise sales quantity (only those products that were sold every week)\n",
    "2. Last 4 total sales quantity\n",
    "3. quarter, month, year, and week of the year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-MK4s4PubVTW"
   },
   "source": [
    "**A. Product wise sales quantity**\n",
    "\n",
    "Grouping the original DataFrame df1 by 'week_start_date' and 'product_item' columns, and aggregating the 'quantity' column by sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "my_VQAbVbZNf"
   },
   "outputs": [],
   "source": [
    "# group by week_number and product_item and sum the quantity column\n",
    "df_sales = df1.groupby(['week_start_date', 'product_item']).agg({'quantity': 'sum'}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VsSoErCAbnrd"
   },
   "source": [
    "Pivoting the df_sales DataFrame to create a new DataFrame df_pivot with each product item as a column and the total sales for each week as the values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vUmmhBsObq8P"
   },
   "outputs": [],
   "source": [
    "df_pivot = df_sales.pivot(index='week_start_date', columns='product_item', values='quantity').reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHvNEHSFb7Q9"
   },
   "source": [
    "Creating a list of product items and merging it with another DataFrame df2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZzTNogt8chBh"
   },
   "outputs": [],
   "source": [
    "products = df1['product_item'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uf25fC4gb9b_"
   },
   "outputs": [],
   "source": [
    "df3 = pd.merge(df_pivot[list(products[products==67].index.values)+['week_start_date']], df2, right_index=True, left_on='week_start_date')\n",
    "df3.set_index('week_start_date', drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99AU0mJkcBcs"
   },
   "source": [
    "This creates a new DataFrame df by merging the df_pivot DataFrame with df2 DataFrame using the 'week_start_date' column as the key. \n",
    "\n",
    "It selects only the columns corresponding to the product items that have a count of 67 in the original df1 DataFrame, and includes the 'week_start_date' column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNGM8SOVdyxh"
   },
   "source": [
    "**B. Creating lag values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Me2k2yIcHSEn"
   },
   "outputs": [],
   "source": [
    "# Shifting the target (quantity) up by 1 so it becomes target (to be predicted) for all the columns\n",
    "df3['quantity'] = df3['quantity'].shift(-1)\n",
    "\n",
    "df3.index.freq = 'W-MON'  # Setting frequency to weekly\n",
    "\n",
    "## Last 4 total sales quantity features\n",
    "for i in range(1, 5):\n",
    "  df3[f'last {i} quantity'] = df3['quantity'].shift(i, fill_value=df3['quantity'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OiprPUkXeASv"
   },
   "source": [
    "**C. Adding time element features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RGIyK2e7d-M5"
   },
   "outputs": [],
   "source": [
    "df3['quarter'] = df3.index.quarter\n",
    "df3['month'] = df3.index.month\n",
    "df3['year'] = df3.index.year\n",
    "df3['weekofyear'] = df3.index.isocalendar().week.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fosc2KQteUUd"
   },
   "outputs": [],
   "source": [
    "# Storing last row before dropping to predict value for May\n",
    "last_row = df3.iloc[-1]\n",
    "df3.drop(df3.index[-1], inplace=True)  # Dropping last value because of shifting target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2rcRbsbMGB6"
   },
   "source": [
    "**2. Visualize our Feature (month) / Target Relationship**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 718
    },
    "id": "VQ-oirS1MMoU",
    "outputId": "78c8dceb-8083-4828-d1ee-274157af4569"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.boxplot(data=df3, x='month', y='quantity')\n",
    "ax.set_title('Quantity Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCFl4m2DgSsM"
   },
   "source": [
    "### G. Train and Test Split ‚ûó"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1erH0HSgWvb"
   },
   "source": [
    "We will split our train and test split with the following ration = ` 80 Train :20 Test`\n",
    "\n",
    "**Note:** I will use `.iloc[:54]` because 54 is 80% of the total dataset (which has 67 rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503
    },
    "id": "sgaq19ATgequ",
    "outputId": "e39bebf0-31a3-47c1-c1a6-91600dd3c231"
   },
   "outputs": [],
   "source": [
    "# Create a train/test split using .iloc [FOR VISUALIZATION]\n",
    "train = df2.iloc[:54]  # Select first 54 rows as training set\n",
    "test = df2.iloc[53:]   # Select remaining rows as test set\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "train.plot(ax=ax, label='Training Set', title='Data Train/Test Split')\n",
    "test.plot(ax=ax, label='Test Set')\n",
    "\n",
    "# Add vertical black line to separate train and test sets\n",
    "ax.axvline(x=train.index[-1], color='black', linestyle='--')\n",
    "\n",
    "ax.legend(['Training Set', 'Test Set'])\n",
    "plt.show()\n",
    "\n",
    "# Create a train/test split using .iloc [WITH NEW FEATURES]\n",
    "train = df3.iloc[:54]  # Select first 54 rows as training set\n",
    "test = df3.iloc[53:]   # Select remaining rows as test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lK8TmNThXqO"
   },
   "source": [
    "**Note:**\n",
    "\n",
    "The reason is that when you use df2.iloc[54:] as the test set, it starts from the 55th row (index 54) onwards, which means it does not include the 54th row. \n",
    "\n",
    "On the other hand, if you use df2.iloc[53:] as the test set, it starts from the 54th row (index 53) onwards, including the 54th row. This ensures that the line chart is connected between the last data point of the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MW83LD5RPXk1"
   },
   "source": [
    "### H. Model Selection and Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3VOZft_xPcYv"
   },
   "source": [
    "**1. Chosen Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GrRsclK2QvP_"
   },
   "source": [
    "1. Baseline Models ‚úÖ\n",
    "\n",
    "  a. `Exponential Smoothing models:` Exponential Smoothing models are a family of statistical models for time series forecasting that capture both trend and seasonality in the data. \n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "2. Complex Models ‚úÖ\n",
    "\n",
    "  a. `ARIMA (AutoRegressive Integrated Moving Average)` ARIMA is a relatively simple model and can be a good choice for stationary data without seasonality or trend.\n",
    "\n",
    "  b. `Gradient Boosting Models (e.g., XGBoost, LightGBM, CatBoost):` Gradient boosting models are powerful machine learning algorithms that can be used for time series forecasting. These models can capture complex patterns in the data and are known for their accuracy and ability to handle large datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlEgVuMASKMf"
   },
   "source": [
    "**I. Defining Features and Target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "un4p_S2RSQpm"
   },
   "outputs": [],
   "source": [
    "X_train = train.drop('quantity', axis=1)\n",
    "y_train = train['quantity']\n",
    "\n",
    "X_test = test.drop('quantity', axis=1)\n",
    "y_test = test['quantity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XT_-_Ui8PgdD"
   },
   "source": [
    "**2. Model Assumptions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-Db2bkJ5Ld2"
   },
   "source": [
    "Checking the stationarity of the data using the dickey fuller test. If the p-value comes below 0.05 then, it is assumed that the data is stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_2n3QUjjnVRW"
   },
   "outputs": [],
   "source": [
    "def check_stationarity(series):\n",
    "    result = adfuller(series.values)\n",
    "\n",
    "    print('ADF Statistic: %f' % result[0])\n",
    "    print('p-value: %f' % result[1])\n",
    "    print('Critical Values:')\n",
    "    for key, value in result[4].items():\n",
    "        print('\\t%s: %.3f' % (key, value))\n",
    "\n",
    "    if (result[1] <= 0.05) & (result[4]['5%'] > result[0]):\n",
    "        print(\"\\u001b[32mStationary\\u001b[0m\")\n",
    "    else:\n",
    "        print(\"\\x1b[31mNon-stationary\\x1b[0m\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qY_QY1YonbZl",
    "outputId": "3e51e4e2-6e69-4b3f-f314-c99b0facdf66"
   },
   "outputs": [],
   "source": [
    "check_stationarity(train['quantity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIhGLxg2aLfb"
   },
   "source": [
    "**Observation:**\n",
    "A negative ADF test statistic with a p-value close to zero and lower than the significance level (commonly set at 0.05) indicates that the time series data is likely stationary, which means it does not have a trend or seasonality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ospCMHRcPjdA"
   },
   "source": [
    "**2. Model Evaluation Metrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cE42r5Ijov-v"
   },
   "source": [
    "Evaluation metric for evaluation of the models is chosen to be root mean squared error. This metric is often used because it penalises the predictions that have too much error. \n",
    "\n",
    "This is what is required in forecasting for sales as we would need to forecast all the values with fair bit of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mw-nvg9zRe00"
   },
   "source": [
    "### I. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWKVukPaA9vW"
   },
   "source": [
    "\n",
    "\n",
    "**(I) Exponential smoothing**\n",
    "<br>Here three models are applied on the dataset and their rmse values are computed:\n",
    "1. Exponential smoothing\n",
    "2. Holt's method with additive model \n",
    "3. Holt-winter's method with additive model\n",
    "<br>Additive model makes sense as the trend appears to be linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 642
    },
    "id": "bgb8jlp-kj58",
    "outputId": "cf3c7963-5182-477d-bf02-9d9b8b80c64d"
   },
   "outputs": [],
   "source": [
    "### Exponential Smoothing Models\n",
    "\n",
    "#######################################################\n",
    "## Exponential Smoothing (Simple Exponential Smoothing)\n",
    "#######################################################\n",
    "# Fit the model\n",
    "model_ses = ExponentialSmoothing(train['quantity'], trend=None, seasonal=None)\n",
    "model_ses_fit = model_ses.fit()\n",
    "\n",
    "# Predict on test set\n",
    "test_pred_ses = model_ses_fit.forecast(len(test)-1)\n",
    "\n",
    "# Create a copy of the test set DataFrame\n",
    "test_pred_df_ses = test.copy()\n",
    "\n",
    "# Set predicted values for Simple Exponential Smoothing\n",
    "test_pred_df_ses['Exponential Smoothing (Simple)'] = test_pred_ses\n",
    "\n",
    "########################################\n",
    "## Exponential Smoothing (Holt's Method)\n",
    "########################################\n",
    "# Fit the model\n",
    "model_holt = ExponentialSmoothing(train['quantity'], trend='add', seasonal=None)\n",
    "model_holt_fit = model_holt.fit()\n",
    "\n",
    "# Predict on test set\n",
    "test_pred_holt = model_holt_fit.forecast(len(test)-1)\n",
    "\n",
    "# Create a copy of the test set DataFrame\n",
    "test_pred_df_holt = test.copy()\n",
    "\n",
    "# Set predicted values for Holt's Method\n",
    "test_pred_df_holt['Exponential Smoothing (Holt)'] = test_pred_holt\n",
    "\n",
    "###############################################\n",
    "## Exponential Smoothing (Holt-Winters' Method)\n",
    "###############################################\n",
    "# Fit the model\n",
    "model_hw = ExponentialSmoothing(train['quantity'], trend='add', seasonal='add', seasonal_periods=3)\n",
    "model_hw_fit = model_hw.fit()\n",
    "\n",
    "# Predict on test set\n",
    "test_pred_hw = model_hw_fit.forecast(len(test)-1)\n",
    "\n",
    "# Create a copy of the test set DataFrame\n",
    "test_pred_df_hw = test.copy()\n",
    "\n",
    "# Set predicted values for Holt-Winters' Method\n",
    "test_pred_df_hw['Exponential Smoothing (Holt-Winters)'] = test_pred_hw\n",
    "\n",
    "\n",
    "## Plotting\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "# Plot the training set\n",
    "train['quantity'].plot(ax=ax, label='Training Set')\n",
    "\n",
    "# Plot the original test set\n",
    "test['quantity'].plot(ax=ax, label='Test Set (Actual)', color='darkorange')\n",
    "\n",
    "# Plot the Exponential Smoothing (Simple) predicted values\n",
    "test_pred_ses.plot(ax=ax, y='Exponential Smoothing (Simple)', label='Exponential Smoothing (Simple)', color='green')\n",
    "\n",
    "# Plot the Exponential Smoothing (Holt) predicted values\n",
    "test_pred_holt.plot(ax=ax, y='Exponential Smoothing (Holt)', label='Exponential Smoothing (Holt)', color='red')\n",
    "\n",
    "# Plot the Exponential Smoothing (Holt-Winters) predicted values\n",
    "test_pred_hw.plot(ax=ax, y='Exponential Smoothing (Holt-Winters)', label='Exponential Smoothing (Holt-Winters)', color='blue')\n",
    "\n",
    "# Set plot title and labels\n",
    "ax.set_title('Exponential Smoothing Forecasting')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Quantity')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "## Model evaluation for test set\n",
    "expon_smooth_rmse = rmse(test_pred_df_ses['Exponential Smoothing (Simple)'].iloc[1:], y_test.iloc[1:])\n",
    "expon_holt_rmse = rmse(test_pred_df_holt['Exponential Smoothing (Holt)'].iloc[1:], y_test.iloc[1:])\n",
    "expon_hw_rmse = rmse(test_pred_df_hw['Exponential Smoothing (Holt-Winters)'].iloc[1:], y_test.iloc[1:])\n",
    "print(\"Root mean squared errors:\")\n",
    "print(\"Exponential smoothing (simple) rmse:\", expon_smooth_rmse)\n",
    "print(\"Exponential smoothing (Holt's method) rmse:\", expon_holt_rmse)\n",
    "print(\"Exponential smoothing (Hol-winter's method) rmse:\", expon_hw_rmse)\n",
    "\n",
    "expon_smooth_mape = mape(test_pred_df_ses['Exponential Smoothing (Simple)'].iloc[1:], y_test.iloc[1:])\n",
    "expon_holt_mape = mape(test_pred_df_holt['Exponential Smoothing (Holt)'].iloc[1:], y_test.iloc[1:])\n",
    "expon_hw_mape = mape(test_pred_df_hw['Exponential Smoothing (Holt-Winters)'].iloc[1:], y_test.iloc[1:])\n",
    "print(\"Mean Absolute Percentage Errors:\")\n",
    "print(\"Exponential smoothing (simple) MAPE:\", expon_smooth_mape)\n",
    "print(\"Exponential smoothing (Holt's method) MAPE:\", expon_holt_mape)\n",
    "print(\"Exponential smoothing (Holt-Winters' method) MAPE:\", expon_hw_mape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLRbyP02XwCe"
   },
   "source": [
    "**(II) ARIMA (AutoRegressive Integrated Moving Average)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IanPW5IftaYU"
   },
   "source": [
    "Running arima model with random p,d,q parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "id": "fx0AjNx5ji3P",
    "outputId": "71f0ed70-e390-4748-92f2-3d6ab6710942"
   },
   "outputs": [],
   "source": [
    "## Create ARIMA model\n",
    "model = ARIMA(train['quantity'], order=(1, 1, 0))  #random 3 values pdq\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_pred = model_fit.predict(start=len(train), end=len(train) + len(test) - 2)\n",
    "\n",
    "# Create a copy of the test set DataFrame\n",
    "test_pred_df = test.copy()\n",
    "\n",
    "# Add the predicted values as a new column to the DataFrame\n",
    "test_pred_df['ARIMA Prediction'] = test_pred\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "train['quantity'].plot(ax=ax, label='Training Set', title='Data Train/Test Split with Model Prediction')\n",
    "test['quantity'].plot(ax=ax, label='Test Set (Actual)', color='darkorange')  # Add the original test set with custom label\n",
    "test_pred.plot(ax=ax, y='ARIMA Prediction', label='Test Set (Prediction)', color='red')  # Plot the predicted values with custom label and color\n",
    "\n",
    "# Add vertical black line to separate train and test sets\n",
    "ax.axvline(x=train.index[-1], color='black', linestyle='--')\n",
    "\n",
    "ax.legend(['Training Set', 'Test Set (Actual)', 'Test Set (Prediction)'])  # Update the legend labels\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Calculate RMSE for ARIMA forecast\n",
    "arima_rmse = rmse(test_pred_df['ARIMA Prediction'].iloc[1:], y_test.iloc[1:])\n",
    "\n",
    "# Calculate MAPE for ARIMA forecast\n",
    "arima_mape = mape(test_pred_df['ARIMA Prediction'].iloc[1:], y_test.iloc[1:])\n",
    "\n",
    "print(\"ARIMA RMSE:\", arima_rmse)\n",
    "print(\"ARIMA MAPE:\", arima_mape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3YIMUdtBF_g"
   },
   "source": [
    "This code block will determine the best p,d,q values for the ARIMA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "deUiqGSLtmgP",
    "outputId": "4512daa7-ac39-4027-d63b-4b94286e4455"
   },
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Function to find the best ARIMA order\n",
    "def find_best_arima_order(train_data, max_order=(5, 2, 5)):\n",
    "    best_rmse = float('inf')\n",
    "    best_order = None\n",
    "\n",
    "    # Loop through possible parameter values for ARIMA\n",
    "    for p in range(max_order[0] + 1):\n",
    "        for d in range(max_order[1] + 1):\n",
    "            for q in range(max_order[2] + 1):\n",
    "                try:\n",
    "                    model = ARIMA(train_data, order=(p, d, q))\n",
    "                    results = model.fit(method_kwargs={'warn_convergence': False})\n",
    "                    # Calculate RMSE\n",
    "                    y_pred = results.predict(start=len(train_data), end=len(train_data) + len(y_test) - 1)\n",
    "                    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "                    if rmse < best_rmse:\n",
    "                        best_rmse = rmse\n",
    "                        best_order = (p, d, q)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "    return best_order, best_rmse\n",
    "\n",
    "# Find the best ARIMA order\n",
    "best_order, best_rmse = find_best_arima_order(y_train)\n",
    "\n",
    "print(\"Best ARIMA Order (p, d, q):\", best_order)\n",
    "print(\"Best RMSE:\", best_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5W4QFaMlDSru"
   },
   "source": [
    "Running the ARIMA model with the best parameters (p,d,q) of (5,1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "id": "ZEd04VtR3iuJ",
    "outputId": "0c2ef84c-ceaa-4eab-f7e0-d573d1ac1ea7"
   },
   "outputs": [],
   "source": [
    "# Create ARIMA model\n",
    "model = ARIMA(train['quantity'], order=(5, 1, 3))  #random 3 values pdq\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_pred = model_fit.predict(start=len(train), end=len(train) + len(test) - 2)\n",
    "\n",
    "# # Create a copy of the test set DataFrame\n",
    "# test_pred_df = test.copy()\n",
    "\n",
    "# Add the predicted values as a new column to the DataFrame\n",
    "test_pred_df['ARIMA Prediction'] = test_pred\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "train['quantity'].plot(ax=ax, label='Training Set', title='Data Train/Test Split with Model Prediction')\n",
    "test['quantity'].plot(ax=ax, label='Test Set (Actual)', color='darkorange')  # Add the original test set with custom label\n",
    "test_pred.plot(ax=ax, y='ARIMA Prediction', label='Test Set (Prediction)', color='red')  # Plot the predicted values with custom label and color\n",
    "\n",
    "# Add vertical black line to separate train and test sets\n",
    "ax.axvline(x=train.index[-1], color='black', linestyle='--')\n",
    "\n",
    "ax.legend(['Training Set', 'Test Set (Actual)', 'Test Set (Prediction)'])  # Update the legend labels\n",
    "plt.show()\n",
    "\n",
    "# Calculate RMSE for ARIMA forecast\n",
    "arima_rmse = rmse(test_pred_df['ARIMA Prediction'].iloc[1:], y_test.iloc[1:])\n",
    "\n",
    "# Calculate MAPE for ARIMA forecast\n",
    "arima_mape = mape(test_pred_df['ARIMA Prediction'].iloc[1:], y_test.iloc[1:])\n",
    "\n",
    "print(\"ARIMA RMSE:\", arima_rmse)\n",
    "print(\"ARIMA MAPE:\", arima_mape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ceVjOThRhoF"
   },
   "source": [
    "**(III) Gradient Boosting Models with XGBoost**\n",
    "<br> Here all the features created are used for the ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "id": "cYC69C1-TdzJ",
    "outputId": "e4b0d5f5-5dec-4b36-8927-fe89860913f6"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "reg = xgb.XGBRegressor(base_score=0.5, booster='gbtree',\n",
    "                       n_estimators=200,\n",
    "                       objective='reg:linear',\n",
    "                       max_depth=2,\n",
    "                       learning_rate=0.03)\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_pred = reg.predict(X_test)\n",
    "\n",
    "\n",
    "# Create a copy of the test set DataFrame\n",
    "test_pred_df = test.copy()\n",
    "\n",
    "# Add the predicted values as a new column to the DataFrame\n",
    "test_pred_df['XGB Boost Prediction'] = test_pred\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "train['quantity'].plot(ax=ax, label='Training Set', title='Data Train/Test Split with Model Prediction')\n",
    "test['quantity'].plot(ax=ax, label='Test Set (Actual)', color='darkorange')  # Add the original test set with custom label\n",
    "test_pred_df.plot(ax=ax, y='XGB Boost Prediction', label='Test Set (Prediction)', color='green')  # Plot the predicted values with custom label and color\n",
    "\n",
    "# Add vertical black line to separate train and test sets\n",
    "ax.axvline(x=train.index[-1], color='black', linestyle='--')\n",
    "\n",
    "ax.legend(['Training Set', 'Test Set (Actual)', 'Test Set (Prediction)'])  # Update the legend labels\n",
    "plt.show()\n",
    "\n",
    "# Calculate RMSE for XGBoost forecast\n",
    "xgboost_rmse = rmse(test_pred_df['XGB Boost Prediction'], y_test)\n",
    "\n",
    "# Calculate MAPE for XGBoost forecast\n",
    "xgboost_mape = mape(test_pred_df['XGB Boost Prediction'], y_test)\n",
    "\n",
    "print(\"XGBoost RMSE:\", xgboost_rmse)\n",
    "print(\"XGBoost MAPE:\", xgboost_mape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWPOj3D3xglG"
   },
   "source": [
    "**(IV) Visualizing top 10 important features for xgboost model**\n",
    "<br>The ML model determines which features were the most important in predicting the result for xgboost model. The following code shows the top 10 features that were more important in the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "YthkDAQ_sWdr",
    "outputId": "19f5df6e-bfec-475b-c292-c8f9816461a9"
   },
   "outputs": [],
   "source": [
    "fi = pd.DataFrame(data=reg.feature_importances_,\n",
    "             index=reg.feature_names_in_,\n",
    "             columns=['importance'])\n",
    "fi.sort_values('importance').tail(10).plot(kind='barh', title='Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4LOD-ODIlb5"
   },
   "source": [
    "## 4. Conclusion and Overall Analysis üîñ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LRNg9qPu2YKd",
    "outputId": "ceefe9a5-4ebd-422e-889c-78925da83b39"
   },
   "outputs": [],
   "source": [
    "print(\"Exponential smoothening Holt method rmse:\", expon_holt_rmse)\n",
    "print(\"Exponential smoothening Holt method rmse:\", expon_holt_mape)\n",
    "\n",
    "print(\"Holt winters method rmse:\", expon_hw_rmse)\n",
    "print(\"Holt winters method mape:\", expon_hw_mape)\n",
    "\n",
    "print(\"ARIMA method rmse:\", arima_rmse)\n",
    "print(\"ARIMA method mape:\", arima_mape)\n",
    "\n",
    "print('xgboost rmse:', xgboost_rmse)\n",
    "print('xgboost mape:', xgboost_mape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gtDIUr8zxYpu"
   },
   "source": [
    "With the above rmse and mape values, it is evident that the `xgboost regressor has the best performance`. This makes sense as the dataset has very few instances to train on.There is also almost no seasonality component in the dataset.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "**1. Why did XGBoost performed better than other models?**\n",
    "\n",
    "The performance of the statistical based forecasting methods like exponential smoothing and ARIMA models have more or less the same performance. This is because there is not much predictability if we consider only the single time series of total sales quantity. However, if we include the individual product sales quantity (like we have done in xgb regressor) as features, the predictions are better as the total sales quantity may have a relation with the individual products sales quantity. \n",
    "\n",
    "- XGBoost is capable of capturing complex non-linear relationships in the data, while ARIMA assumes linear relationships between variables.\n",
    "- XGBoost can handle larger datasets with a large number of features, while ARIMA may struggle with computational limitations.\n",
    "- XGBoost allows for the incorporation of external features, while ARIMA relies only on the historical values of the time series itself.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "**2. Why I did not use SARIMA or ARIMAX**\n",
    "\n",
    "Sarima only introduces a seasonal component in arima. As we have established there is not seasonality so, we don't use it. \n",
    "\n",
    "I think ARIMAX is exogenous variable based where there is a variable that we know will impact the sales in future beforehand, like a holiday or something. I have never used it. There was no variable like that here.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "**3. Overall Analyis**\n",
    "\n",
    "The performance of the xgb regressor is more or less around 550000 rmse value. If we compare it with average value of the dataset which is around 5,000,000, it is at `10% error level`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlR4sRCyxyqj"
   },
   "source": [
    "**Note:**\n",
    "\n",
    "Business analysis written in the slide deck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIh0CID_ltgy"
   },
   "source": [
    "**Predictions for May 2023**\n",
    "<br>The following code shows the predictions for May"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g-zhac0RFxEt",
    "outputId": "2fa8d39b-ac25-445d-a287-12e876fdf44e"
   },
   "outputs": [],
   "source": [
    "print(\"Exponential smoothing holt's method:\", model_holt_fit.forecast(len(test))[-1])\n",
    "print(\"Exponential smoothing holt-winter's method:\", model_hw_fit.forecast(len(test))[-1])\n",
    "print(\"ARIMA model:\", model_fit.predict(start=len(train), end=len(train) + len(test) - 1)[-1])\n",
    "print(\"xgb regressor prediction: \", reg.predict(pd.DataFrame(last_row).T.drop('quantity',axis=1))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjIp62JKsB0w"
   },
   "source": [
    "## 5. Saving the model üõí\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t7orBk5hxm4U"
   },
   "outputs": [],
   "source": [
    "# Save the trained model to a file\n",
    "with open('xgboost_model.pkl', 'wb') as file:\n",
    "    pickle.dump(reg, file)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "kqzIPtoOTGDm",
    "OvprTXmpTSZK",
    "aTpWuJGjh48f",
    "yWlM6BOMuYt1"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
